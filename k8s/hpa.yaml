apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: mlops-inference-hpa
  labels:
    app: mlops-inference
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: mlops-inference
  
  minReplicas: 2   # Minimum 2 pods for high availability
  maxReplicas: 10  # Maximum 10 pods to prevent runaway scaling
  
  metrics:
  # Scale based on CPU utilization
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70  # Target 70% CPU across all pods
  
  # Scale based on memory utilization
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80  # Target 80% memory across all pods
  
  # Scaling behavior configuration
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60  # Wait 60s before scaling up
      policies:
      - type: Percent
        value: 50          # Scale up by 50% of current pods
        periodSeconds: 60  # Every 60 seconds
      - type: Pods
        value: 2           # Or add 2 pods at a time
        periodSeconds: 60  # Whichever is larger
      selectPolicy: Max    # Use the policy that scales faster
    
    scaleDown:
      stabilizationWindowSeconds: 300  # Wait 5 minutes before scaling down
      policies:
      - type: Pods
        value: 1           # Remove 1 pod at a time
        periodSeconds: 60  # Every 60 seconds
      selectPolicy: Min    # Use the policy that scales slower
